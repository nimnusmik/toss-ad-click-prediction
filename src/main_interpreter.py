#%%
#!/usr/bin/env python3
"""
DCN ëª¨ë¸ì„ ì‚¬ìš©í•œ í´ë¦­ ì˜ˆì¸¡ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
ë°ì´í„° ë¡œë”©, í›ˆë ¨, ì¶”ë¡ , ì œì¶œ íŒŒì¼ ìƒì„±ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""
import os
import sys
import torch

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import average_precision_score, log_loss
from torch.utils.data import DataLoader
from tqdm import tqdm

from config import CFG, device
from data_loader import load_data, get_feature_columns, ClickDataset, collate_fn_train
from train import train_dcn_model
from inference import load_model, predict, create_submission


# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("DCN Click Prediction Pipeline")
    print("=" * 80)
    
    # 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
    print("\n1. Loading and preprocessing data...")
    train_df, test_df = load_data(CFG['DATA_PATH'])
    numeric_cols, categorical_info, seq_col, target_col = get_feature_columns(train_df)
    
    # 2. ëª¨ë¸ í›ˆë ¨
    print(f"\n2. Training DCN model...")
    print(f"   - Batch size: {CFG['BATCH_SIZE']}")
    print(f"   - Epochs: {CFG['EPOCHS']}")
    print(f"   - Learning rate: {CFG['LEARNING_RATE']}")
    print(f"   - Device: {device}")
    
    model = train_dcn_model(
        train_df=train_df,
        numeric_cols=numeric_cols,
        categorical_info=categorical_info,
        seq_col=seq_col,
        target_col=target_col,
        batch_size=CFG['BATCH_SIZE'],
        epochs=20,
        lr=CFG['LEARNING_RATE'],
        device=device
    )
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    
    # 3. ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±
    print(f"\n3. Making predictions and creating submission...")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ pandasë¡œ ë³€í™˜
    test_pd = test_df.to_pandas()
    
    # ëª¨ë¸ ë¡œë“œ (í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©)
    model = load_model(CFG['MODEL_PATH'], numeric_cols, categorical_info, device)
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    test_preds = predict(
        model=model,
        test_df=test_pd,
        numeric_cols=numeric_cols,
        categorical_info=categorical_info,
        seq_col=seq_col,
        batch_size=CFG['BATCH_SIZE'],
        device=device
    )
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    os.makedirs(CFG['OUTPUT_PATH'], exist_ok=True)
    #submission = create_submission(
    #    test_preds=test_preds,
    #    sample_submission_path='../data/sample_submission.csv',
    #    output_path=f"{CFG['OUTPUT_PATH']}dcn_submission.csv"
    #)
    
    print(f"\n4. Pipeline completed successfully!")
    #print(f"   - Submission file: {CFG['OUTPUT_PATH']}dcn_submission.csv")
    print(f"   - Predictions shape: {test_preds.shape}")
    print(f"   - Prediction range: [{test_preds.min():.4f}, {test_preds.max():.4f}]")
    
    return model, test_preds, submission, train_df, numeric_cols, categorical_info, seq_col, target_col


    
#%%
model, predictions, submission, train_df, numeric_cols, categorical_info, seq_col, target_col = main()


#%%
# ê°„ë‹¨í•œ ëª¨ë¸ ë¶„ì„ ì‹¤í–‰
def simple_model_analysis(model, train_df, numeric_cols, categorical_info, seq_col, target_col):
    """ê°„ë‹¨í•œ ëª¨ë¸ í•´ì„ ë¶„ì„"""
    print("\n" + "=" * 80)
    print("ğŸ” SIMPLE MODEL ANALYSIS")
    print("=" * 80)
    
    # 1. ê²€ì¦ ë°ì´í„° ì¤€ë¹„ (ê°„ë‹¨í•˜ê²Œ)
    print("ğŸ“Š ê²€ì¦ ë°ì´í„° ì¤€ë¹„...")
    train_pd = train_df.to_pandas() if hasattr(train_df, 'to_pandas') else train_df
    
    # ìƒ˜í”Œë§ìœ¼ë¡œ ë¹ ë¥´ê²Œ (ì „ì²´ ë°ì´í„° ëŒ€ì‹  ì¼ë¶€ë§Œ)
    sample_df = train_pd.sample(n=min(5000000, len(train_pd)), random_state=42)
    _, val_df = train_test_split(sample_df, test_size=0.3, random_state=42)
    
    val_dataset = ClickDataset(val_df, numeric_cols, seq_col, target_col, categorical_info=categorical_info, has_target=True)
    val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False, collate_fn=collate_fn_train)
    
    print(f"   ë¶„ì„ ë°ì´í„°: {len(val_df):,}ê°œ ìƒ˜í”Œ")
    
    # 2. ê¸°ë³¸ ì„±ëŠ¥ í‰ê°€
    print("\nğŸ¯ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€...")
    model.eval()
    all_probs, all_targets = [], []
    correct, total = 0, 0
    
    with torch.no_grad():
        for x_num, x_cat, seqs, seq_lens, ys in tqdm(val_loader, desc="ì„±ëŠ¥ í‰ê°€"):
            x_num = x_num.to(device)
            x_cat = x_cat.to(device) if x_cat is not None else None
            seqs = seqs.to(device)
            seq_lens = seq_lens.to(device)
            ys = ys.to(device)

            logits = model(x_num, x_cat, seqs, seq_lens)
            probs = torch.sigmoid(logits)
            predicted = (probs > 0.5).float()
            
            total += ys.size(0)
            correct += (predicted == ys).sum().item()
            
            all_probs.extend(probs.cpu().numpy())
            all_targets.extend(ys.cpu().numpy())
    
    accuracy = correct / total
    all_probs = np.array(all_probs)
    all_targets = np.array(all_targets)
    
    # ëŒ€íšŒ í‰ê°€ì§€í‘œ ê³„ì‚°
    ap_score = average_precision_score(all_targets, all_probs)
    
    # ê°€ì¤‘ ë¡œê·¸ ì†ì‹¤
    class_counts = np.bincount(all_targets.astype(int))
    total_samples = len(all_targets)
    weight_0 = 0.5 / (class_counts[0] / total_samples) if class_counts[0] > 0 else 1.0
    weight_1 = 0.5 / (class_counts[1] / total_samples) if class_counts[1] > 0 else 1.0
    sample_weights = np.where(all_targets, weight_1, weight_0)
    wll_score = log_loss(all_targets, all_probs, sample_weight=sample_weights)
    
    final_score = (ap_score + (1 - wll_score)) / 2
    
    print(f"\nğŸ“ˆ ì„±ëŠ¥ ê²°ê³¼:")
    print(f"   ì •í™•ë„: {accuracy:.4f}")
    print(f"   í‰ê·  ì˜ˆì¸¡ í™•ë¥ : {np.mean(all_probs):.4f}")
    print(f"   ì‹¤ì œ í´ë¦­ë¥ : {np.mean(all_targets):.4f}")
    print(f"\nğŸ† ëŒ€íšŒ í‰ê°€ì§€í‘œ:")
    print(f"   AP (Average Precision): {ap_score:.4f}")
    print(f"   WLL (Weighted LogLoss): {wll_score:.4f}")
    print(f"   Final Score: {final_score:.4f}")
    
    # 3. ê°„ë‹¨í•œ í”¼ì²˜ ì¤‘ìš”ë„ (ë¹ ë¥¸ ë²„ì „)
    print(f"\nğŸ” í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ (Top 10)...")
    
    # ì›ë˜ ì„±ëŠ¥
    original_score = log_loss(all_targets, all_probs)
    feature_importance = []
    
    # ìƒìœ„ 30ê°œ í”¼ì²˜ë§Œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
    important_features = numeric_cols[:30]  
    
    for feat_idx in tqdm(range(len(important_features)), desc="í”¼ì²˜ ì¤‘ìš”ë„"):
        # ì‘ì€ ìƒ˜í”Œë¡œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
        test_probs = []
        
        with torch.no_grad():
            for batch_idx, (x_num, x_cat, seqs, seq_lens, ys) in enumerate(val_loader):
                if batch_idx >= 5:  # 5ê°œ ë°°ì¹˜ë§Œ
                    break
                x_num = x_num.to(device)
                x_cat = x_cat.to(device) if x_cat is not None else None
                seqs = seqs.to(device)
                seq_lens = seq_lens.to(device)

                # í”¼ì²˜ permutation
                x_perm = x_num.clone()
                perm_indices = torch.randperm(x_perm.size(0))
                x_perm[:, feat_idx] = x_perm[perm_indices, feat_idx]

                logits = model(x_perm, x_cat, seqs, seq_lens)
                probs = torch.sigmoid(logits)
                test_probs.extend(probs.cpu().numpy())
        
        if len(test_probs) > 0:
            test_targets = all_targets[:len(test_probs)]
            permuted_score = log_loss(test_targets, test_probs)
            importance = permuted_score - original_score
        else:
            importance = 0
            
        feature_importance.append((important_features[feat_idx], importance))
    
    # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    feature_importance.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\nğŸ† Top 10 ì¤‘ìš” í”¼ì²˜:")
    for i, (feat_name, importance) in enumerate(feature_importance[:10]):
        print(f"   {i+1:2d}. {feat_name:<25}: {importance:.6f}")
    
    # 4. ì˜ˆì¸¡ ë¶„í¬ ì‹œê°í™”
    print(f"\nğŸ“Š ì˜ˆì¸¡ ë¶„í¬ ì‹œê°í™”...")
    
    plt.figure(figsize=(20, 5))
    
    # ì „ì²´ ë¶„í¬
    plt.subplot(1, 4, 1)
    plt.hist(all_probs, bins=30, alpha=0.7, color='blue', edgecolor='black')
    plt.title('Prediction Probability Distribution')
    plt.xlabel('Prediction Probability')
    plt.ylabel('Frequency')
    
    # í´ë˜ìŠ¤ë³„ ë¶„í¬
    plt.subplot(1, 4, 2)
    clicked_probs = all_probs[all_targets == 1]
    not_clicked_probs = all_probs[all_targets == 0]
    
    plt.hist(not_clicked_probs, bins=20, alpha=0.7, label='Not Clicked (0)', color='red')
    plt.hist(clicked_probs, bins=20, alpha=0.7, label='Clicked (1)', color='green')
    plt.title('Class-wise Prediction Distribution')
    plt.xlabel('Prediction Probability')
    plt.ylabel('Frequency')
    plt.legend()
    
    # ê³ ì‹ ë¢°ë„ ì˜ˆì¸¡ vs ì‹¤ì œ í´ë¦­ ë¹„êµ
    plt.subplot(1, 4, 3)
    
    # 0.6 ì´ìƒ ì˜ˆì¸¡í•œ ìƒ˜í”Œë“¤
    high_pred_mask = all_probs >= 0.6
    high_pred_samples = all_probs[high_pred_mask]
    high_pred_actual = all_targets[high_pred_mask]
    
    # ì‹¤ì œ clicked=1ì¸ ìƒ˜í”Œë“¤ì˜ ì˜ˆì¸¡ í™•ë¥ 
    actual_clicked_probs = all_probs[all_targets == 1]
    
    # íˆìŠ¤í† ê·¸ë¨ ë¹„êµ
    plt.hist(actual_clicked_probs, bins=20, alpha=0.7, label=f'Actual Clicked=1 ({len(actual_clicked_probs)})', 
            color='green', density=True)
    plt.hist(high_pred_samples, bins=20, alpha=0.7, label=f'Predicted â‰¥0.6 ({len(high_pred_samples)})', 
            color='orange', density=True)
    
    plt.axvline(x=0.6, color='red', linestyle='--', alpha=0.8, label='Threshold 0.6')
    plt.title('High Confidence Predictions vs Actual Clicks')
    plt.xlabel('Prediction Probability')
    plt.ylabel('Density')
    plt.legend()
    
    # ì„±ëŠ¥ ìš”ì•½
    plt.subplot(1, 4, 4)
    
    # êµ¬ê°„ë³„ ë¶„ì„
    thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]
    precision_scores = []
    recall_scores = []
    
    for threshold in thresholds:
        pred_mask = all_probs >= threshold
        if np.sum(pred_mask) > 0:
            # Precision: ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ ë§ì€ ë¹„ìœ¨
            precision = np.mean(all_targets[pred_mask])
            # Recall: ì‹¤ì œ í´ë¦­ ì¤‘ ì°¾ì•„ë‚¸ ë¹„ìœ¨  
            recall = np.sum(all_targets[pred_mask]) / np.sum(all_targets)
        else:
            precision = 0
            recall = 0
        
        precision_scores.append(precision)
        recall_scores.append(recall)
    
    x_pos = np.arange(len(thresholds))
    width = 0.35
    
    bars1 = plt.bar(x_pos - width/2, precision_scores, width, label='Precision', color='lightblue')
    bars2 = plt.bar(x_pos + width/2, recall_scores, width, label='Recall', color='lightcoral')
    
    plt.xlabel('Threshold')
    plt.ylabel('Score')
    plt.title('Precision vs Recall by Threshold')
    plt.xticks(x_pos, [f'{t:.1f}' for t in thresholds])
    plt.legend()
    plt.ylim(0, 1)
    
    # ê°’ í‘œì‹œ
    for bar, score in zip(bars1, precision_scores):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{score:.3f}', ha='center', va='bottom', fontsize=8)
    for bar, score in zip(bars2, recall_scores):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{score:.3f}', ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    plt.show()
    
    # ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ¯ ê³ ì‹ ë¢°ë„ ì˜ˆì¸¡ ë¶„ì„ (ì„ê³„ê°’ 0.6):")
    high_pred_count = np.sum(high_pred_mask)
    high_pred_correct = np.sum(high_pred_actual)
    actual_clicked_count = np.sum(all_targets)
    
    if high_pred_count > 0:
        precision_06 = high_pred_correct / high_pred_count
        recall_06 = high_pred_correct / actual_clicked_count
        print(f"   ì˜ˆì¸¡ â‰¥0.6ì¸ ìƒ˜í”Œ: {high_pred_count:,}ê°œ")
        print(f"   ì´ ì¤‘ ì‹¤ì œ í´ë¦­: {high_pred_correct:,}ê°œ")
        print(f"   ì •ë°€ë„(Precision): {precision_06:.4f}")
        print(f"   ì¬í˜„ìœ¨(Recall): {recall_06:.4f}")
    else:
        print(f"   ì˜ˆì¸¡ â‰¥0.6ì¸ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"\nğŸ“Š ì‹¤ì œ í´ë¦­(1) ìƒ˜í”Œë“¤ì˜ ì˜ˆì¸¡ ë¶„í¬:")
    print(f"   ì´ ì‹¤ì œ í´ë¦­: {actual_clicked_count:,}ê°œ")
    print(f"   ì´ ì¤‘ 0.6+ ì˜ˆì¸¡: {np.sum(actual_clicked_probs >= 0.6):,}ê°œ ({np.mean(actual_clicked_probs >= 0.6):.3f})")
    print(f"   ì´ ì¤‘ 0.8+ ì˜ˆì¸¡: {np.sum(actual_clicked_probs >= 0.8):,}ê°œ ({np.mean(actual_clicked_probs >= 0.8):.3f})")
    print(f"   í‰ê·  ì˜ˆì¸¡ í™•ë¥ : {np.mean(actual_clicked_probs):.4f}")
    
    # 5. ìµœì  ì„ê³„ê°’ ì°¾ê¸°
    from sklearn.metrics import f1_score
    
    best_f1 = 0
    best_threshold = 0.5
    
    for threshold in np.arange(0.1, 0.9, 0.05):
        pred_binary = (all_probs >= threshold).astype(int)
        f1 = f1_score(all_targets, pred_binary)
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    print(f"\nğŸ¯ ìµœì í™” ê²°ê³¼:")
    print(f"   ìµœì  ì„ê³„ê°’: {best_threshold:.3f}")
    print(f"   ìµœì  F1 ì ìˆ˜: {best_f1:.4f}")
    
    print(f"\nâœ… ê°„ë‹¨ ë¶„ì„ ì™„ë£Œ!")
    
    return {
        'accuracy': accuracy,
        'ap_score': ap_score,
        'wll_score': wll_score,
        'final_score': final_score,
        'best_threshold': best_threshold,
        'top_features': [f[0] for f in feature_importance[:5]]
    }

print(f"\nğŸ¤” ëª¨ë¸ì´ ì–´ë–»ê²Œ íŒë‹¨í•˜ê³  ìˆëŠ”ì§€ ê°„ë‹¨íˆ ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤...")

analysis_results = simple_model_analysis(
    model=model,
    train_df=train_df,
    numeric_cols=numeric_cols,
    categorical_info=categorical_info,
    seq_col=seq_col,
    target_col=target_col
)

if analysis_results:
    print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
    print(f"   ğŸ“Š Final Score: {analysis_results['final_score']:.4f}")
    print(f"   ğŸ¯ ìµœì  ì„ê³„ê°’: {analysis_results['best_threshold']:.3f}")
    print(f"   ğŸ† Top 5 ì¤‘ìš” í”¼ì²˜: {', '.join(analysis_results['top_features'])}")
    
else:
    print(f"\nâš ï¸ ë¶„ì„ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆì§€ë§Œ ëª¨ë¸ í›ˆë ¨ê³¼ ì œì¶œ íŒŒì¼ì€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

print(f"\nğŸ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
print(f"   ğŸ“ ì œì¶œ íŒŒì¼: {CFG['OUTPUT_PATH']}dcn_submission.csv")
print(f"   ğŸ’¾ ëª¨ë¸ íŒŒì¼: {CFG['MODEL_PATH']}")


# %%
